{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEMMueXzUmhJ0S9e7YB0iQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritiyadav888/AI-projects/blob/main/YouTube_Video_Q%26A_with_Gemini_on_Vertex_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Video Q&A with Gemini on Vertex AI\n",
        "\n",
        "This application provides an intelligent way to interact with YouTube video content by allowing users to ask questions and receive AI-generated answers based on the video's transcript.\n",
        "\n",
        "## Key Features:\n",
        "\n",
        "* **Question Answering:** Users can input a YouTube video URL and then pose specific questions about the video's content. The app intelligently retrieves relevant sections from the video's transcript to formulate a precise answer using advanced AI.\n",
        "* **Vertex AI Integration:** The app seamlessly integrates with Google Cloud's Vertex AI platform. This integration ensures access to cutting-edge and regularly updated AI models, providing robust and scalable capabilities.\n",
        "* **Gemini Models:**\n",
        "    * **LLM (Large Language Model):** The application utilizes `gemini-2.0-flash-001` (with `gemini-pro` as a reliable fallback) to comprehend the user's question and synthesize a coherent answer derived directly from the video transcript.\n",
        "    * **Embedding Model:** `gemini-embedding-001` (with `textembedding-gecko` as a fallback) is employed to transform both the user's question and the video transcript's content into numerical representations (embeddings). These embeddings are crucial for enabling efficient and accurate retrieval of relevant information.\n",
        "* **Transcript Processing:** The app automates the fetching of video transcripts using Langchain's `YoutubeLoader`. Once obtained, the transcript is intelligently split into smaller, manageable chunks, optimizing the retrieval process. The processing is cached to improve performance for repeated queries.\n",
        "* **Semantic Search:** A vector database, Chroma, is used to store the embeddings of the transcript chunks. This enables \"semantic search,\" meaning the app can find parts of the transcript that are conceptually similar to the user's question, even if the exact keywords are not present.\n",
        "* **Gradio Interface:** A user-friendly web-based interface is provided by Gradio, enabling a chatbot-like experience. Users can enter a YouTube URL to automatically load the video transcript, ask multiple questions without reprocessing, and quit by typing \"quit\" or clicking a \"Quit\" button. The interface provides real-time status updates and supports markdown-formatted answers with timestamp links where applicable.\n",
        "\n",
        "## Target Audience:\n",
        "\n",
        "This application is designed to benefit a diverse group of users, including:\n",
        "\n",
        "* **General Users:** Anyone seeking quick, direct answers from video content without the need to watch the entire duration.\n",
        "\n",
        "## Purpose:\n",
        "\n",
        "The primary objective of this application is to revolutionize how users consume information from videos. It directly addresses the challenge of manually sifting through extensive video content to find particular details. By intelligently processing video transcripts, implementing caching for efficiency, and offering an interactive chatbot interface, the app empowers users to gain insights rapidly and effortlessly."
      ],
      "metadata": {
        "id": "vuVBMk5U_bfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JsbdnbNo72V3",
        "outputId": "a4bce10d-935a-47ea-8686-c313df19668d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-core<1.0.0,>=0.3.65 (from langchain-community)\n",
            "  Using cached langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
            "  Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Using cached langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
            "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: pydantic-core, pydantic, langchain-core\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.2\n",
            "    Uninstalling pydantic_core-2.18.2:\n",
            "      Successfully uninstalled pydantic_core-2.18.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.59\n",
            "    Uninstalling langchain-core-0.3.59:\n",
            "      Successfully uninstalled langchain-core-0.3.59\n",
            "Successfully installed langchain-core-0.3.65 pydantic-2.11.7 pydantic-core-2.33.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core",
                  "pydantic"
                ]
              },
              "id": "09144fdb7bcd4742afc6f9355b92c851"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-aiplatform langchain-google-vertexai pydantic==2.7.1 youtube-transcript-api chromadb gradio"
      ],
      "metadata": {
        "id": "RpDAuJOzABqU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "cZywELyE9kqW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && !apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194gWsAwNabe",
        "outputId": "4b146243-3845-4042-97f7-528b326adaa2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [79.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,250 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,296 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,986 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,040 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,557 kB]\n",
            "Fetched 23.1 MB in 6s (3,568 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "/bin/bash: line 1: !apt: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install whisper\n",
        "!pip install yt-dlp\n",
        "!apt install ffmpeg  # Required for audio extraction with yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3OEahJ1NucD",
        "outputId": "9e2efbae-6019-439b-a2ea-476f4a930864"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Using cached whisper-1.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.6.9)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall whisper -y\n",
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "id": "GZXruhRHOT0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage\n",
        "import gradio as gr\n",
        "import os\n",
        "import re\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from joblib import Memory"
      ],
      "metadata": {
        "id": "FiIUQJBKHWXe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Replace with your actual Google Cloud Project ID and desired region\n",
        "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "# Initialize Vertex AI\n",
        "print(f\"Initializing Vertex AI with project: {PROJECT_ID}, location: {REGION}\")\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# --- Initialize Models and Global State ---\n",
        "llm_model = None\n",
        "embedding_model = None\n",
        "embeddings_for_chroma = None\n",
        "global_retriever = None  # Store the retriever for the loaded video\n",
        "current_video_url = None  # Track the currently loaded video\n",
        "memory = Memory(\"cache_directory\", verbose=0)  # Initialize joblib cache\n",
        "\n",
        "# Attempt to load LLM: gemini-2.0-flash-001, fallback to gemini-pro if not found\n",
        "try:\n",
        "    llm_model = GenerativeModel(\"gemini-2.0-flash-001\")\n",
        "    print(\"Attempting to load gemini-2.0-flash-001 for LLM.\")\n",
        "    _ = llm_model.generate_content([Part.from_text(\"Hello\")], generation_config={\"max_output_tokens\": 10})\n",
        "    print(\"Successfully loaded and tested gemini-2.0-flash-001 for LLM.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not fully load or access gemini-2.0-flash-001. Error: {e}. Falling back to gemini-pro.\")\n",
        "    try:\n",
        "        llm_model = GenerativeModel(\"gemini-pro\")\n",
        "        _ = llm_model.generate_content([Part.from_text(\"Hello\")], generation_config={\"max_output_tokens\": 10})\n",
        "        print(\"Successfully loaded and tested gemini-pro for LLM.\")\n",
        "    except Exception as e_fallback:\n",
        "        print(f\"CRITICAL ERROR: Could not load gemini-pro either. Error: {e_fallback}. Please check project permissions and model availability.\")\n",
        "        llm_model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiJy6UcTE7gx",
        "outputId": "17670a99-c1db-435e-bef1-572c9602e2df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Vertex AI with project: data-read-421406, location: us-central1\n",
            "Attempting to load gemini-2.0-flash-001 for LLM.\n",
            "Successfully loaded and tested gemini-2.0-flash-001 for LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to load Embedding Model: gemini-embedding-001, fallback to textembedding-gecko\n",
        "try:\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")\n",
        "    print(\"Successfully loaded gemini-embedding-001 for embeddings.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load gemini-embedding-001. Error: {e}. Falling back to textembedding-gecko.\")\n",
        "    try:\n",
        "        embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
        "        print(\"Successfully loaded textembedding-gecko for embeddings.\")\n",
        "    except Exception as e_fallback:\n",
        "        print(f\"CRITICAL ERROR: Could not load textembedding-gecko either. Error: {e_fallback}. Please check project permissions and model availability.\")\n",
        "        embedding_model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiRb1lybE9wO",
        "outputId": "cc1b41ca-5024-49b3-b5d7-2a7d03deaafd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded gemini-embedding-001 for embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only proceed with creating embedding wrapper if embedding_model loaded successfully\n",
        "if embedding_model:\n",
        "    # --- Embedding Function for ChromaDB ---\n",
        "    class VertexAIGeminiEmbeddings:\n",
        "        def __init__(self, model_instance):\n",
        "            self.model = model_instance\n",
        "            self.output_dimensionality = 768  # Enforce 768 dimensions for Chroma compatibility\n",
        "\n",
        "        def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
        "            batch_size = 50  # Process texts in batches to reduce API calls\n",
        "            embeddings = []\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i + batch_size]\n",
        "                text_inputs = [TextEmbeddingInput(text, \"RETRIEVAL_DOCUMENT\") for text in batch]\n",
        "                kwargs = {\"output_dimensionality\": self.output_dimensionality}\n",
        "                try:\n",
        "                    embedding_response = self.model.get_embeddings(text_inputs, **kwargs)\n",
        "                    embeddings.extend([r.values if r.values else [0.0] * self.output_dimensionality for r in embedding_response])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during batch embedding documents: {e}\")\n",
        "                    embeddings.extend([[0.0] * self.output_dimensionality] * len(batch))\n",
        "            return embeddings\n",
        "\n",
        "        def embed_query(self, text: str) -> list[float]:\n",
        "            text_input = TextEmbeddingInput(text, \"RETRIEVAL_QUERY\")\n",
        "            kwargs = {\"output_dimensionality\": self.output_dimensionality}\n",
        "            try:\n",
        "                embedding_response = self.model.get_embeddings([text_input], **kwargs)\n",
        "                if embedding_response and embedding_response[0].values:\n",
        "                    return embedding_response[0].values\n",
        "                return [0.0] * self.output_dimensionality\n",
        "            except Exception as e:\n",
        "                print(f\"Error during embedding query: {e}\")\n",
        "                return [0.0] * self.output_dimensionality\n",
        "\n",
        "    embeddings_for_chroma = VertexAIGeminiEmbeddings(embedding_model)\n",
        "else:\n",
        "    print(\"Embeddings model not loaded, Q&A functionality will be limited.\")\n",
        "    embeddings_for_chroma = None"
      ],
      "metadata": {
        "id": "u29jNLDgFABP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate YouTube URL\n",
        "def validate_youtube_url(url):\n",
        "    youtube_regex = r'^(https?://)?(www\\.)?(youtube\\.com|youtu\\.be)/.+$'\n",
        "    return bool(re.match(youtube_regex, url))\n",
        "\n",
        "# Function to Process YouTube Video\n",
        "@memory.cache\n",
        "def process_video(video_url):\n",
        "    global global_retriever, current_video_url\n",
        "    if not embeddings_for_chroma:\n",
        "        raise ValueError(\"Embedding model not initialized. Cannot process video.\")\n",
        "\n",
        "    if not validate_youtube_url(video_url):\n",
        "        raise ValueError(\"Invalid YouTube URL. Please provide a valid YouTube video URL.\")\n",
        "\n",
        "    # Extract video ID from playlist URL if present\n",
        "    video_id = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11})', video_url)\n",
        "    if not video_id:\n",
        "        raise ValueError(\"Could not extract video ID from URL. Please provide a valid video URL.\")\n",
        "    video_id = video_id.group(1)\n",
        "    print(f\"Extracted video ID: {video_id}\")\n",
        "\n",
        "    if \"list=\" in video_url:\n",
        "        print(f\"Playlist URL detected. Processing video ID: {video_id} instead of playlist.\")\n",
        "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"  # Use the specific video URL\n",
        "\n",
        "    print(f\"Loading YouTube video transcript for: {video_url}...\")\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    try:\n",
        "        from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "        @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "        def load_with_retry():\n",
        "            return loader.load()\n",
        "        docs = load_with_retry()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load public transcript: {str(e)}. Falling back to Whisper...\")\n",
        "        import whisper\n",
        "        import yt_dlp\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "            'outtmpl': f'temp_audio_{video_id}.mp3',  # Unique filename per video\n",
        "            'quiet': False,  # Enable verbose output for debugging\n",
        "        }\n",
        "        try:\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                print(f\"Downloading audio for video ID: {video_id}...\")\n",
        "                ydl.download([video_url])\n",
        "            model = whisper.load_model(\"medium\")  # Should work with openai-whisper\n",
        "            print(\"Transcribing audio with Whisper...\")\n",
        "            result = model.transcribe(f'temp_audio_{video_id}.mp3')  # Auto-detect language\n",
        "            print(f\"Detected language: {result['language']}\")\n",
        "            print(f\"Transcribed text: {result['text'][:200]}...\")  # Log first 200 chars for debugging\n",
        "            docs = [result[\"text\"]]\n",
        "            os.remove(f'temp_audio_{video_id}.mp3')  # Clean up temporary file\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load audio: {str(e)}. This video may have restricted audio or an issue with ffmpeg/yt-dlp. Please try another video.\")\n",
        "            raise ValueError(\"Audio extraction failed. Please try a different video.\")\n",
        "\n",
        "    if not docs:\n",
        "        raise ValueError(\"This video does not have a public transcript, and Whisper transcription failed. Please try another video.\")\n",
        "\n",
        "    print(f\"Loaded {len(docs)} document(s) from transcript.\")\n",
        "    print(\"Splitting documents into chunks...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    print(f\"Split into {len(chunks)} chunks.\")\n",
        "\n",
        "    print(\"Creating Chroma vectorstore with embeddings...\")\n",
        "    vectorstore = Chroma.from_documents(chunks, embedding=embeddings_for_chroma)\n",
        "    print(\"Chroma vectorstore created.\")\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    global_retriever = retriever\n",
        "    current_video_url = video_url\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "0xH5-WPZFBB4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Ask Question\n",
        "def ask_question(video_url: str, question: str, chat_history: list):\n",
        "    global global_retriever, current_video_url\n",
        "    if llm_model is None or embeddings_for_chroma is None:\n",
        "        chat_history.append([\"Bot\", \"Error: LLM or Embedding model failed to initialize.\"])\n",
        "        return chat_history\n",
        "\n",
        "    # Check for quit command\n",
        "    if question.lower().strip() == \"quit\":\n",
        "        chat_history.append([\"User\", \"quit\"])\n",
        "        chat_history.append([\"Bot\", \"Conversation ended. You can load a new video or ask more questions.\"])\n",
        "        global_retriever = None\n",
        "        current_video_url = None\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # Reset and reprocess if URL changes\n",
        "        if global_retriever and video_url != current_video_url:\n",
        "            global_retriever = None\n",
        "            chat_history.append([\"Bot\", f\"Switching to new video: {video_url}...\"])\n",
        "        # Process video if not yet loaded or URL changed\n",
        "        if not global_retriever:\n",
        "            chat_history.append([\"Bot\", f\"Processing video: {video_url}...\"])\n",
        "            retriever = process_video(video_url)\n",
        "        else:\n",
        "            retriever = global_retriever\n",
        "\n",
        "        print(f\"Retrieving relevant documents for question: '{question}'...\")\n",
        "        docs = retriever.get_relevant_documents(question)\n",
        "        print(f\"Found {len(docs)} relevant documents.\")\n",
        "\n",
        "        # Take top 2 for context\n",
        "        context = \"\"\n",
        "        timestamps = []\n",
        "        if docs:\n",
        "            for doc in docs[:2]:\n",
        "                content = doc.page_content\n",
        "                timestamp_matches = re.findall(r'(\\d+:\\d+)', content)\n",
        "                if timestamp_matches:\n",
        "                    timestamps.extend(timestamp_matches)\n",
        "                context += content + \"\\n\"\n",
        "        else:\n",
        "            chat_history.append([\"User\", question])\n",
        "            chat_history.append([\"Bot\", \"No relevant context found in the video transcript for your question.\"])\n",
        "            return chat_history\n",
        "\n",
        "        print(\"\\nConstructing prompt for LLM...\")\n",
        "        prompt_parts = [\n",
        "            Part.from_text(\"You are a helpful AI assistant. Based on the video transcript below, answer the user's question in a clear and concise manner.\"),\n",
        "            Part.from_text(f\"\\nContext: {context}\"),\n",
        "            Part.from_text(f\"\\nQuestion: {question}\"),\n",
        "            Part.from_text(\"\\nAnswer:\")\n",
        "        ]\n",
        "\n",
        "        print(\"Invoking LLM to get response...\")\n",
        "        response = llm_model.generate_content(\n",
        "            prompt_parts,\n",
        "            generation_config={\"temperature\": 0.2, \"max_output_tokens\": 512}\n",
        "        )\n",
        "        print(\"\\n--- LLM Response ---\")\n",
        "\n",
        "        # Format response with markdown\n",
        "        formatted_response = f\"**Answer:**\\n{response.text}\"\n",
        "        if timestamps:\n",
        "            video_id = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11})', video_url)\n",
        "            if video_id:\n",
        "                for ts in timestamps:\n",
        "                    minutes, seconds = map(int, ts.split(':'))\n",
        "                    total_seconds = minutes * 60 + seconds\n",
        "                    formatted_response += f\"\\n- [Jump to {ts}](https://www.youtube.com/watch?v={video_id.group(1)}&t={total_seconds}s)\"\n",
        "\n",
        "        chat_history.append([\"User\", question])\n",
        "        chat_history.append([\"Bot\", formatted_response])\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append([\"User\", question])\n",
        "        chat_history.append([\"Bot\", f\"Error during Q&A: {str(e)}\"])\n",
        "        return chat_history"
      ],
      "metadata": {
        "id": "ETbiErldFD_N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gradio Interface ---\n",
        "if llm_model is None or embeddings_for_chroma is None:\n",
        "    print(\"\\nSkipping Gradio interface launch due to critical model loading errors.\")\n",
        "else:\n",
        "    print(\"\\nLaunching Gradio Interface...\")\n",
        "\n",
        "    def process_video_on_change(video_url, chat_history):\n",
        "        global global_retriever, current_video_url\n",
        "        try:\n",
        "            if not validate_youtube_url(video_url):\n",
        "                chat_history.append([\"Bot\", \"Invalid YouTube URL. Please provide a valid YouTube video URL.\"])\n",
        "                return chat_history, \"Error\"\n",
        "            # Reset if URL changes\n",
        "            if global_retriever and video_url != current_video_url:\n",
        "                global_retriever = None\n",
        "                chat_history.append([\"Bot\", f\"Switching to new video: {video_url}...\"])\n",
        "            if not global_retriever:\n",
        "                chat_history.append([\"Bot\", f\"Loading video transcript for: {video_url}...\"])\n",
        "                retriever = process_video(video_url)\n",
        "                chat_history.append([\"Bot\", f\"Video loaded: {video_url}. You can now ask questions. Type 'quit' to end the conversation.\"])\n",
        "            return chat_history, \"Video loaded\"\n",
        "        except Exception as e:\n",
        "            chat_history.append([\"Bot\", f\"Error loading video: {str(e)}\"])\n",
        "            return chat_history, \"Error\"\n",
        "\n",
        "    def quit_conversation(chat_history):\n",
        "        global global_retriever, current_video_url\n",
        "        chat_history.append([\"Bot\", \"Conversation ended. You can load a new video or ask more questions.\"])\n",
        "        global_retriever = None\n",
        "        current_video_url = None\n",
        "        return [], \"Ready\"  # Return empty chat history to refresh\n",
        "\n",
        "    with gr.Blocks() as interface:\n",
        "        gr.Markdown(\"# YouTube Video Q&A with Gemini on Vertex AI\")\n",
        "        gr.Markdown(\"Enter a YouTube video URL to load it automatically and ask questions about its content. Type 'quit' or click 'Quit' to end the conversation.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                video_url_input = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"e.g., https://www.youtube.com/watch?v=1lm4Wlpy2wU\")\n",
        "            with gr.Column(scale=1):\n",
        "                status_output = gr.Textbox(label=\"Status\", value=\"Ready\", interactive=False)\n",
        "\n",
        "        chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n",
        "        question_input = gr.Textbox(label=\"Ask a Question\", placeholder=\"e.g., How do you get a transcript from a YouTube video?\")\n",
        "        with gr.Row():\n",
        "            ask_button = gr.Button(\"Ask\")\n",
        "            quit_button = gr.Button(\"Quit\")\n",
        "\n",
        "        chat_history = gr.State(value=[])\n",
        "\n",
        "\n",
        "        # Trigger video processing when URL changes\n",
        "        video_url_input.change(\n",
        "            fn=process_video_on_change,\n",
        "            inputs=[video_url_input, chat_history],\n",
        "            outputs=[chatbot, status_output]\n",
        "        )\n",
        "        ask_button.click(ask_question, inputs=[video_url_input, question_input, chat_history], outputs=[chatbot])\n",
        "        quit_button.click(quit_conversation, inputs=[chat_history], outputs=[chatbot, status_output])\n",
        "\n",
        "    interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "hcIvWogHFGht",
        "outputId": "b0c738a7-3474-458a-dd5d-285a3f77d1d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Launching Gradio Interface...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-965442899>:43: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a91035599c93431507.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a91035599c93431507.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
